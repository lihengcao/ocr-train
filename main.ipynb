{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST\n",
      "FashionMNIST\n",
      "KMNIST\n",
      "MNIST\n",
      "MovingMNIST\n",
      "QMNIST\n",
      "mnist\n",
      "moving_mnist\n"
     ]
    }
   ],
   "source": [
    "a = dir(datasets)\n",
    "\n",
    "for text in a:\n",
    "    if \"mnist\" in text.lower():\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset EMNIST\n",
      "    Number of datapoints: 697932\n",
      "    Root location: ./dataset/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: <function TRANSFORM_CALLABLE at 0x7f77c47d5ea0>\n",
      "Dataset EMNIST\n",
      "    Number of datapoints: 116323\n",
      "    Root location: ./dataset/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: <function TRANSFORM_CALLABLE at 0x7f77c47d5ea0>\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"./dataset/\"\n",
    "\n",
    "# merges letters that have similar upper and lower cases, like C:c and W:w\n",
    "SPLIT_DATA_BY = \"bymerge\"\n",
    "\n",
    "\n",
    "def TRANSFORM_CALLABLE(img: Image.Image) -> torch.Tensor:\n",
    "    return ToTensor()(img.transpose(Image.TRANSPOSE))\n",
    "\n",
    "training_data = datasets.EMNIST(root=ROOT_DIR, split=SPLIT_DATA_BY, train=True, download=True, transform=TRANSFORM_CALLABLE)\n",
    "testing_data = datasets.EMNIST(ROOT_DIR, split=SPLIT_DATA_BY, train=False, download=True, transform=TRANSFORM_CALLABLE)\n",
    "\n",
    "print(training_data)\n",
    "print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAMMUlEQVR4nO3czYuWdd/H8e/pTI2YTJZZlOTUrhzLRUKUSAZtDCQky432F7QIahuBFFTrFm2LchdUIBREpeIDRFChIEZhheAUOmMJ+TAz57W578/dfT3A/I4rz3NwXq9l+eE8HMbeHonfXr/f7xcAVNWyYT8AAIuHKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiwJL11Vdf1fPPP1+Tk5N100031bp16+rZZ5+tU6dODfvRYGh6bh+xVO3cubMOHz5czzzzTD344IN19uzZeuutt+rixYt17Nix2rBhw7AfEQZOFFiyjhw5Ups2baobb7wx/+z777+vBx54oHbu3FnvvffeEJ8OhkMU4J889NBDVVX19ddfD/lJYPD8mQL8Rb/fr6mpqbrtttuG/SgwFKIAf/H+++/XmTNnateuXcN+FBgK//sI/sfJkyfr4YcfrsnJyTp06FCNjIwM+5Fg4EQBqurs2bO1efPmunr1ah07dqzuuuuuYT8SDMXosB8Ahu3ChQu1bdu2mpmZqUOHDgkCS5oosKRdunSptm/fXqdOnarPPvus1q9fP+xHgqESBZasubm52rVrVx09erQ++uijeuSRR4b9SDB0osCS9eKLL9bHH39c27dvr/Pnz//LX1bbvXv3kJ4MhscfNLNkbd26tQ4cOPAf/71fGixFogBA+MtrAIQoABCiAECIAgAhCgCEKAAQC/7La71e71o+BwDX2EL+BoI3BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAGB32AzB8o6PX37fB/Pz8QDZwvfGmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABDX3yW060Sv1+u0m5iYaN7s2LGjeTM+Pt686arLobrjx483b06cONG8OX/+fPPm3LlzzZuqqn6/32kHLbwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAESvv8ArW10PtNHN6tWrO+3eeOON5s2uXbuaN2NjY82brrocgpuZmWneTE9PN2+OHDnSvNm7d2/zpqrq9OnTnXbwvxbya8mbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECMDvsB+Pe6HiBcuXJl82b58uXNm5GRkebN1atXmzdVVWfOnOm0azUxMdG8Wbt2bfOm62G7N998s3lz6dKlTp/F0uVNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwJXWRmp6e7rTbt29f8+bee+9t3qxatap50/Xa6d69e5s3Fy9ebN68/PLLzZsnn3yyebN9+/bmTVXVu+++27zpepGVpcubAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEA4iLdIzc3Nddp9+umnzZvvvvuueTM62v6tMzs727yp6nZIr9frNW++/fbb5s22bduaN+Pj482bqm5fc2jlTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgXNi6zly+fLl5c/r06b//QYbs9ttvb95MTk42b5Yta/991fz8fPMGBsWbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEA4iMeiNzIy0rzZsmVL82bz5s3Nmy5OnDjRaXfhwoW/+UngX3lTACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBcSWVgbrjhhk679evXN2/27NnTvFm9enXz5s8//2zeHD16tHlTVTU9Pd1pBy28KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEg3h0MjY21rzZtGlTp896/fXXB/JZV65cad58+OGHzZsPPvigeVNVNTs722kHLbwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAISDeNSKFSuaNzt27GjevPTSS82bqqr777+/efP77783b/bv39+8efXVV5s3P/30U/OG/7NsWfvvZbts+v1+82Zubq55s9h4UwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIB/EWqZGRkU67iYmJ5s2ePXuaN88991zzpsuzVVX99ttvzZu33357IJtff/21edPl0NogdTkeNzY21ry58847mzdVVRs3bmzebNiwoXkzPT3dvNm3b1/zpqrq/PnznXbXgjcFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAQr1Gv12verFmzpnmzZcuW5k1V1e7du5s3TzzxRPNm+fLlzZsff/yxeVNV9dprrzVv9u/f37w5d+5c86bL4cIu30NVVatWrWre3HLLLc2bycnJ5s2jjz7avHn88cebN1VVd999d/NmfHy8eXPq1KnmzSeffNK8qXIQD4BFShQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYklfSR0bG2ve3Hfffc2bF154oXmzbdu25k1V1erVq5s3V65cad50uQb5zjvvNG+qqj7//PPmTZermOvWrWvePPbYY82bLtdOq7pdL92wYUPzpstl1a4/py6mp6ebN12u5h48eLB5s5iunXblTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgev1+v7+gH9jrXetn+a8+Z82aNc2bLkfnuhy363JEr8uxvqqqP/74o3nz5ZdfNm9+/vnn5k2XQ2ZVVStWrGjebN26tXlz6623Nm/uuOOO5s2yZYP7vdj8/HzzZmpqqnnT5RDcF1980bypqjpy5Ejz5vDhw82bLj+n2dnZ5s0gLeQ/994UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGLRHcS75557Ou1eeeWV5k2Xg3hdDu91+dp1/XpfvXq1eXPu3Lnmzc0339y8GR0dbd5UdTvqNqjv1y5H/rocZ6uqOn78ePNmZmameXPgwIHmTZfjcV0O71VVXb58uXnT5XvoeuQgHgBNRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIbhfKFmhkZKR589RTT3X6rKeffrp5s3Llyk6fNQhzc3MD+6zx8fHmzdmzZ5s3XQ6ZVVWdOHFiIJvZ2dnmTZcjdV0P4nU5vrfAe5f/T5evA9cPbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAcU0P4nU5xvXDDz90+qyTJ082byYmJpo3K1asaN5MTU01b7755pvmTVW3Q3AzMzPNm4MHDzZvLly40Lzpuuvyc+ry/To/Pz+QDQyKNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAotdf4GnIXq93rZ+lqqrGxsY67dauXdu82bhxY/Omy2XVLhdFf/nll+ZN1eCug87OzjZvgOFayK91bwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAsegO4g3SsmXtTezydZibm2veAPzdHMQDoIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADE67AcYpvn5+WE/AsCi4k0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBGF/oD+/3+tXwOABYBbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEPwDU6DZL2SgvEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    rand_ind = torch.randint(len(training_data), size=(1,)).item()\n",
    "    assert isinstance(rand_ind, int)\n",
    "\n",
    "    img, label = training_data[rand_ind]\n",
    "    img_class = training_data.classes[label]\n",
    "    \n",
    "    assert isinstance(img, torch.Tensor) and img.shape == (1, 28, 28)\n",
    "\n",
    "    plt.imshow(img[0], cmap=\"gray\")\n",
    "    plt.title(img_class)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.float32\n",
      "torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "X, y = next(iter(testing_dataloader))\n",
    "\n",
    "print(X.shape, X.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModel(\n",
      "  (sequential): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=16, out_features=47, bias=True)\n",
      "    (10): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TheModel(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__() \n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 16, 5, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # nn.Conv2d(16, 16, 3, 2, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            # nn.MaxPool2d(2),\n",
    "\n",
    "            # nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, output_dim),\n",
    "            nn.LogSoftmax(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "model = TheModel(len(training_data.classes)).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= 1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    N = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for ind_batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        forward = model(X)\n",
    "        loss = loss_func(forward, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if ind_batch % 100 == 0:\n",
    "            loss_val, current = loss.item(), (ind_batch + 1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  {current:>5d}/{N:>5d}\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_func):\n",
    "    N = len(dataloader.dataset)\n",
    "    average_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            inference = model(X)\n",
    "            average_loss += loss_func(inference, y).item()\n",
    "            correct += (inference.argmax(1) == y).sum().item()\n",
    "    \n",
    "    average_loss /= len(dataloader)\n",
    "    correct /= N\n",
    "\n",
    "    print(f\"% correct: {(100 * correct):>0.1f}  avg loss: {average_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "---\n",
      "loss: 0.422208     64/697932\n",
      "loss: 0.622322   6464/697932\n",
      "loss: 0.455471  12864/697932\n",
      "loss: 0.421827  19264/697932\n",
      "loss: 0.680950  25664/697932\n",
      "loss: 0.397191  32064/697932\n",
      "loss: 0.451836  38464/697932\n",
      "loss: 0.503272  44864/697932\n",
      "loss: 0.487237  51264/697932\n",
      "loss: 0.441317  57664/697932\n",
      "loss: 0.489513  64064/697932\n",
      "loss: 0.229295  70464/697932\n",
      "loss: 0.647417  76864/697932\n",
      "loss: 0.396058  83264/697932\n",
      "loss: 0.551208  89664/697932\n",
      "loss: 0.423896  96064/697932\n",
      "loss: 0.541708  102464/697932\n",
      "loss: 0.492252  108864/697932\n",
      "loss: 0.585815  115264/697932\n",
      "loss: 0.595073  121664/697932\n",
      "loss: 0.624442  128064/697932\n",
      "loss: 0.500425  134464/697932\n",
      "loss: 0.401156  140864/697932\n",
      "loss: 0.526779  147264/697932\n",
      "loss: 0.615571  153664/697932\n",
      "loss: 0.485353  160064/697932\n",
      "loss: 0.478137  166464/697932\n",
      "loss: 0.416307  172864/697932\n",
      "loss: 0.787634  179264/697932\n",
      "loss: 0.486986  185664/697932\n",
      "loss: 0.346880  192064/697932\n",
      "loss: 0.571324  198464/697932\n",
      "loss: 0.589746  204864/697932\n",
      "loss: 0.577883  211264/697932\n",
      "loss: 0.525345  217664/697932\n",
      "loss: 0.408065  224064/697932\n",
      "loss: 0.511883  230464/697932\n",
      "loss: 0.579069  236864/697932\n",
      "loss: 0.394199  243264/697932\n",
      "loss: 0.564656  249664/697932\n",
      "loss: 0.511564  256064/697932\n",
      "loss: 0.565216  262464/697932\n",
      "loss: 0.466548  268864/697932\n",
      "loss: 0.554711  275264/697932\n",
      "loss: 0.463027  281664/697932\n",
      "loss: 0.695362  288064/697932\n",
      "loss: 0.629331  294464/697932\n",
      "loss: 0.459840  300864/697932\n",
      "loss: 0.482450  307264/697932\n",
      "loss: 0.641359  313664/697932\n",
      "loss: 0.736693  320064/697932\n",
      "loss: 0.438023  326464/697932\n",
      "loss: 0.763130  332864/697932\n",
      "loss: 0.577363  339264/697932\n",
      "loss: 0.811430  345664/697932\n",
      "loss: 0.653793  352064/697932\n",
      "loss: 0.614041  358464/697932\n",
      "loss: 0.401054  364864/697932\n",
      "loss: 0.478590  371264/697932\n",
      "loss: 0.640030  377664/697932\n",
      "loss: 0.385934  384064/697932\n",
      "loss: 0.479472  390464/697932\n",
      "loss: 0.415230  396864/697932\n",
      "loss: 0.434242  403264/697932\n",
      "loss: 0.508368  409664/697932\n",
      "loss: 0.486962  416064/697932\n",
      "loss: 0.675762  422464/697932\n",
      "loss: 0.562170  428864/697932\n",
      "loss: 0.348114  435264/697932\n",
      "loss: 0.672774  441664/697932\n",
      "loss: 0.411755  448064/697932\n",
      "loss: 0.404463  454464/697932\n",
      "loss: 0.407447  460864/697932\n",
      "loss: 0.377325  467264/697932\n",
      "loss: 0.412324  473664/697932\n",
      "loss: 0.403080  480064/697932\n",
      "loss: 0.387365  486464/697932\n",
      "loss: 0.399675  492864/697932\n",
      "loss: 0.651157  499264/697932\n",
      "loss: 0.391179  505664/697932\n",
      "loss: 0.250192  512064/697932\n",
      "loss: 0.392103  518464/697932\n",
      "loss: 0.492660  524864/697932\n",
      "loss: 0.481495  531264/697932\n",
      "loss: 0.709274  537664/697932\n",
      "loss: 0.749273  544064/697932\n",
      "loss: 0.364612  550464/697932\n",
      "loss: 0.390395  556864/697932\n",
      "loss: 0.629815  563264/697932\n",
      "loss: 0.406862  569664/697932\n",
      "loss: 0.410076  576064/697932\n",
      "loss: 0.636939  582464/697932\n",
      "loss: 0.528916  588864/697932\n",
      "loss: 0.432062  595264/697932\n",
      "loss: 0.302788  601664/697932\n",
      "loss: 0.419974  608064/697932\n",
      "loss: 0.794222  614464/697932\n",
      "loss: 0.720705  620864/697932\n",
      "loss: 0.474976  627264/697932\n",
      "loss: 0.621790  633664/697932\n",
      "loss: 0.381287  640064/697932\n",
      "loss: 0.469274  646464/697932\n",
      "loss: 0.500399  652864/697932\n",
      "loss: 0.488705  659264/697932\n",
      "loss: 0.579616  665664/697932\n",
      "loss: 0.568966  672064/697932\n",
      "loss: 0.538966  678464/697932\n",
      "loss: 0.508938  684864/697932\n",
      "loss: 0.399457  691264/697932\n",
      "loss: 0.216793  697664/697932\n",
      "% correct: 83.8  avg loss: 0.501353\n",
      "Epoch 1\n",
      "---\n",
      "loss: 0.413844     64/697932\n",
      "loss: 0.633887   6464/697932\n",
      "loss: 0.436103  12864/697932\n",
      "loss: 0.410659  19264/697932\n",
      "loss: 0.683616  25664/697932\n",
      "loss: 0.392369  32064/697932\n",
      "loss: 0.426319  38464/697932\n",
      "loss: 0.492559  44864/697932\n",
      "loss: 0.478608  51264/697932\n",
      "loss: 0.420474  57664/697932\n",
      "loss: 0.467258  64064/697932\n",
      "loss: 0.218489  70464/697932\n",
      "loss: 0.632226  76864/697932\n",
      "loss: 0.382581  83264/697932\n",
      "loss: 0.551171  89664/697932\n",
      "loss: 0.424032  96064/697932\n",
      "loss: 0.521269  102464/697932\n",
      "loss: 0.486840  108864/697932\n",
      "loss: 0.576004  115264/697932\n",
      "loss: 0.589449  121664/697932\n",
      "loss: 0.619291  128064/697932\n",
      "loss: 0.482637  134464/697932\n",
      "loss: 0.391950  140864/697932\n",
      "loss: 0.518802  147264/697932\n",
      "loss: 0.595980  153664/697932\n",
      "loss: 0.468945  160064/697932\n",
      "loss: 0.472762  166464/697932\n",
      "loss: 0.415839  172864/697932\n",
      "loss: 0.781062  179264/697932\n",
      "loss: 0.480266  185664/697932\n",
      "loss: 0.338275  192064/697932\n",
      "loss: 0.550092  198464/697932\n",
      "loss: 0.563880  204864/697932\n",
      "loss: 0.577377  211264/697932\n",
      "loss: 0.516282  217664/697932\n",
      "loss: 0.398936  224064/697932\n",
      "loss: 0.494534  230464/697932\n",
      "loss: 0.577367  236864/697932\n",
      "loss: 0.385929  243264/697932\n",
      "loss: 0.563739  249664/697932\n",
      "loss: 0.510638  256064/697932\n",
      "loss: 0.546768  262464/697932\n",
      "loss: 0.474753  268864/697932\n",
      "loss: 0.552939  275264/697932\n",
      "loss: 0.454505  281664/697932\n",
      "loss: 0.686282  288064/697932\n",
      "loss: 0.628343  294464/697932\n",
      "loss: 0.459632  300864/697932\n",
      "loss: 0.477310  307264/697932\n",
      "loss: 0.629120  313664/697932\n",
      "loss: 0.719481  320064/697932\n",
      "loss: 0.431562  326464/697932\n",
      "loss: 0.738741  332864/697932\n",
      "loss: 0.559558  339264/697932\n",
      "loss: 0.800101  345664/697932\n",
      "loss: 0.639966  352064/697932\n",
      "loss: 0.593580  358464/697932\n",
      "loss: 0.395740  364864/697932\n",
      "loss: 0.463175  371264/697932\n",
      "loss: 0.638698  377664/697932\n",
      "loss: 0.382850  384064/697932\n",
      "loss: 0.459041  390464/697932\n",
      "loss: 0.415326  396864/697932\n",
      "loss: 0.415532  403264/697932\n",
      "loss: 0.511056  409664/697932\n",
      "loss: 0.478910  416064/697932\n",
      "loss: 0.669768  422464/697932\n",
      "loss: 0.559573  428864/697932\n",
      "loss: 0.346859  435264/697932\n",
      "loss: 0.642447  441664/697932\n",
      "loss: 0.396970  448064/697932\n",
      "loss: 0.402850  454464/697932\n",
      "loss: 0.395365  460864/697932\n",
      "loss: 0.362980  467264/697932\n",
      "loss: 0.399507  473664/697932\n",
      "loss: 0.382724  480064/697932\n",
      "loss: 0.381474  486464/697932\n",
      "loss: 0.391095  492864/697932\n",
      "loss: 0.643679  499264/697932\n",
      "loss: 0.386392  505664/697932\n",
      "loss: 0.248775  512064/697932\n",
      "loss: 0.383477  518464/697932\n",
      "loss: 0.499873  524864/697932\n",
      "loss: 0.486944  531264/697932\n",
      "loss: 0.698100  537664/697932\n",
      "loss: 0.755569  544064/697932\n",
      "loss: 0.343740  550464/697932\n",
      "loss: 0.387070  556864/697932\n",
      "loss: 0.622256  563264/697932\n",
      "loss: 0.393381  569664/697932\n",
      "loss: 0.398917  576064/697932\n",
      "loss: 0.632885  582464/697932\n",
      "loss: 0.509702  588864/697932\n",
      "loss: 0.429109  595264/697932\n",
      "loss: 0.298998  601664/697932\n",
      "loss: 0.405993  608064/697932\n",
      "loss: 0.789452  614464/697932\n",
      "loss: 0.719637  620864/697932\n",
      "loss: 0.464877  627264/697932\n",
      "loss: 0.616327  633664/697932\n",
      "loss: 0.382323  640064/697932\n",
      "loss: 0.463237  646464/697932\n",
      "loss: 0.492679  652864/697932\n",
      "loss: 0.484866  659264/697932\n",
      "loss: 0.580687  665664/697932\n",
      "loss: 0.570879  672064/697932\n",
      "loss: 0.526432  678464/697932\n",
      "loss: 0.499112  684864/697932\n",
      "loss: 0.382024  691264/697932\n",
      "loss: 0.214646  697664/697932\n",
      "% correct: 83.9  avg loss: 0.494849\n",
      "Epoch 2\n",
      "---\n",
      "loss: 0.410328     64/697932\n",
      "loss: 0.640293   6464/697932\n",
      "loss: 0.422846  12864/697932\n",
      "loss: 0.403901  19264/697932\n",
      "loss: 0.680215  25664/697932\n",
      "loss: 0.391339  32064/697932\n",
      "loss: 0.411439  38464/697932\n",
      "loss: 0.495960  44864/697932\n",
      "loss: 0.465665  51264/697932\n",
      "loss: 0.408753  57664/697932\n",
      "loss: 0.445192  64064/697932\n",
      "loss: 0.211616  70464/697932\n",
      "loss: 0.612134  76864/697932\n",
      "loss: 0.363869  83264/697932\n",
      "loss: 0.548597  89664/697932\n",
      "loss: 0.418014  96064/697932\n",
      "loss: 0.505411  102464/697932\n",
      "loss: 0.480973  108864/697932\n",
      "loss: 0.561103  115264/697932\n",
      "loss: 0.581983  121664/697932\n",
      "loss: 0.606189  128064/697932\n",
      "loss: 0.474748  134464/697932\n",
      "loss: 0.381307  140864/697932\n",
      "loss: 0.514810  147264/697932\n",
      "loss: 0.591907  153664/697932\n",
      "loss: 0.458371  160064/697932\n",
      "loss: 0.470543  166464/697932\n",
      "loss: 0.413258  172864/697932\n",
      "loss: 0.779213  179264/697932\n",
      "loss: 0.467171  185664/697932\n",
      "loss: 0.333868  192064/697932\n",
      "loss: 0.527889  198464/697932\n",
      "loss: 0.547559  204864/697932\n",
      "loss: 0.584970  211264/697932\n",
      "loss: 0.510335  217664/697932\n",
      "loss: 0.387681  224064/697932\n",
      "loss: 0.482832  230464/697932\n",
      "loss: 0.582359  236864/697932\n",
      "loss: 0.382073  243264/697932\n",
      "loss: 0.556287  249664/697932\n",
      "loss: 0.509108  256064/697932\n",
      "loss: 0.535527  262464/697932\n",
      "loss: 0.475373  268864/697932\n",
      "loss: 0.550277  275264/697932\n",
      "loss: 0.437583  281664/697932\n",
      "loss: 0.679136  288064/697932\n",
      "loss: 0.625354  294464/697932\n",
      "loss: 0.462677  300864/697932\n",
      "loss: 0.471460  307264/697932\n",
      "loss: 0.619037  313664/697932\n",
      "loss: 0.708335  320064/697932\n",
      "loss: 0.430085  326464/697932\n",
      "loss: 0.719880  332864/697932\n",
      "loss: 0.542799  339264/697932\n",
      "loss: 0.787056  345664/697932\n",
      "loss: 0.639315  352064/697932\n",
      "loss: 0.578648  358464/697932\n",
      "loss: 0.389234  364864/697932\n",
      "loss: 0.447676  371264/697932\n",
      "loss: 0.644254  377664/697932\n",
      "loss: 0.376329  384064/697932\n",
      "loss: 0.445332  390464/697932\n",
      "loss: 0.417948  396864/697932\n",
      "loss: 0.401518  403264/697932\n",
      "loss: 0.506184  409664/697932\n",
      "loss: 0.467817  416064/697932\n",
      "loss: 0.663753  422464/697932\n",
      "loss: 0.561264  428864/697932\n",
      "loss: 0.352032  435264/697932\n",
      "loss: 0.620005  441664/697932\n",
      "loss: 0.382827  448064/697932\n",
      "loss: 0.398575  454464/697932\n",
      "loss: 0.389246  460864/697932\n",
      "loss: 0.353407  467264/697932\n",
      "loss: 0.385285  473664/697932\n",
      "loss: 0.369708  480064/697932\n",
      "loss: 0.374615  486464/697932\n",
      "loss: 0.385795  492864/697932\n",
      "loss: 0.637809  499264/697932\n",
      "loss: 0.377341  505664/697932\n",
      "loss: 0.244737  512064/697932\n",
      "loss: 0.370860  518464/697932\n",
      "loss: 0.507958  524864/697932\n",
      "loss: 0.489454  531264/697932\n",
      "loss: 0.684837  537664/697932\n",
      "loss: 0.755262  544064/697932\n",
      "loss: 0.324204  550464/697932\n",
      "loss: 0.376160  556864/697932\n",
      "loss: 0.626260  563264/697932\n",
      "loss: 0.387832  569664/697932\n",
      "loss: 0.389561  576064/697932\n",
      "loss: 0.637321  582464/697932\n",
      "loss: 0.488032  588864/697932\n",
      "loss: 0.417277  595264/697932\n",
      "loss: 0.296107  601664/697932\n",
      "loss: 0.394963  608064/697932\n",
      "loss: 0.784500  614464/697932\n",
      "loss: 0.722802  620864/697932\n",
      "loss: 0.453847  627264/697932\n",
      "loss: 0.616319  633664/697932\n",
      "loss: 0.372923  640064/697932\n",
      "loss: 0.446288  646464/697932\n",
      "loss: 0.482753  652864/697932\n",
      "loss: 0.486973  659264/697932\n",
      "loss: 0.570257  665664/697932\n",
      "loss: 0.567794  672064/697932\n",
      "loss: 0.520324  678464/697932\n",
      "loss: 0.500854  684864/697932\n",
      "loss: 0.370999  691264/697932\n",
      "loss: 0.210697  697664/697932\n",
      "% correct: 84.1  avg loss: 0.487882\n",
      "Epoch 3\n",
      "---\n",
      "loss: 0.409681     64/697932\n",
      "loss: 0.652536   6464/697932\n",
      "loss: 0.412561  12864/697932\n",
      "loss: 0.401726  19264/697932\n",
      "loss: 0.679848  25664/697932\n",
      "loss: 0.389257  32064/697932\n",
      "loss: 0.397204  38464/697932\n",
      "loss: 0.494500  44864/697932\n",
      "loss: 0.457092  51264/697932\n",
      "loss: 0.395014  57664/697932\n",
      "loss: 0.428320  64064/697932\n",
      "loss: 0.207382  70464/697932\n",
      "loss: 0.601717  76864/697932\n",
      "loss: 0.354873  83264/697932\n",
      "loss: 0.546236  89664/697932\n",
      "loss: 0.418227  96064/697932\n",
      "loss: 0.488688  102464/697932\n",
      "loss: 0.477236  108864/697932\n",
      "loss: 0.556141  115264/697932\n",
      "loss: 0.579555  121664/697932\n",
      "loss: 0.608593  128064/697932\n",
      "loss: 0.464039  134464/697932\n",
      "loss: 0.380878  140864/697932\n",
      "loss: 0.513961  147264/697932\n",
      "loss: 0.583361  153664/697932\n",
      "loss: 0.443960  160064/697932\n",
      "loss: 0.465206  166464/697932\n",
      "loss: 0.409164  172864/697932\n",
      "loss: 0.768470  179264/697932\n",
      "loss: 0.459096  185664/697932\n",
      "loss: 0.333871  192064/697932\n",
      "loss: 0.510295  198464/697932\n",
      "loss: 0.535112  204864/697932\n",
      "loss: 0.591749  211264/697932\n",
      "loss: 0.501738  217664/697932\n",
      "loss: 0.377304  224064/697932\n",
      "loss: 0.468916  230464/697932\n",
      "loss: 0.578913  236864/697932\n",
      "loss: 0.379499  243264/697932\n",
      "loss: 0.559860  249664/697932\n",
      "loss: 0.500907  256064/697932\n",
      "loss: 0.524039  262464/697932\n",
      "loss: 0.478169  268864/697932\n",
      "loss: 0.544720  275264/697932\n",
      "loss: 0.422798  281664/697932\n",
      "loss: 0.674360  288064/697932\n",
      "loss: 0.621184  294464/697932\n",
      "loss: 0.461078  300864/697932\n",
      "loss: 0.460842  307264/697932\n",
      "loss: 0.609197  313664/697932\n",
      "loss: 0.696915  320064/697932\n",
      "loss: 0.429740  326464/697932\n",
      "loss: 0.708391  332864/697932\n",
      "loss: 0.530477  339264/697932\n",
      "loss: 0.784227  345664/697932\n",
      "loss: 0.632454  352064/697932\n",
      "loss: 0.562994  358464/697932\n",
      "loss: 0.387664  364864/697932\n",
      "loss: 0.438343  371264/697932\n",
      "loss: 0.639367  377664/697932\n",
      "loss: 0.376232  384064/697932\n",
      "loss: 0.435686  390464/697932\n",
      "loss: 0.416275  396864/697932\n",
      "loss: 0.389508  403264/697932\n",
      "loss: 0.506153  409664/697932\n",
      "loss: 0.457486  416064/697932\n",
      "loss: 0.652957  422464/697932\n",
      "loss: 0.567404  428864/697932\n",
      "loss: 0.352398  435264/697932\n",
      "loss: 0.606435  441664/697932\n",
      "loss: 0.376504  448064/697932\n",
      "loss: 0.399798  454464/697932\n",
      "loss: 0.384102  460864/697932\n",
      "loss: 0.350461  467264/697932\n",
      "loss: 0.374028  473664/697932\n",
      "loss: 0.361900  480064/697932\n",
      "loss: 0.366454  486464/697932\n",
      "loss: 0.380962  492864/697932\n",
      "loss: 0.628753  499264/697932\n",
      "loss: 0.370479  505664/697932\n",
      "loss: 0.244958  512064/697932\n",
      "loss: 0.363068  518464/697932\n",
      "loss: 0.505964  524864/697932\n",
      "loss: 0.498141  531264/697932\n",
      "loss: 0.679522  537664/697932\n",
      "loss: 0.758584  544064/697932\n",
      "loss: 0.312685  550464/697932\n",
      "loss: 0.366312  556864/697932\n",
      "loss: 0.626958  563264/697932\n",
      "loss: 0.380668  569664/697932\n",
      "loss: 0.374552  576064/697932\n",
      "loss: 0.640649  582464/697932\n",
      "loss: 0.471285  588864/697932\n",
      "loss: 0.410806  595264/697932\n",
      "loss: 0.291136  601664/697932\n",
      "loss: 0.388245  608064/697932\n",
      "loss: 0.771860  614464/697932\n",
      "loss: 0.718763  620864/697932\n",
      "loss: 0.453802  627264/697932\n",
      "loss: 0.614842  633664/697932\n",
      "loss: 0.369989  640064/697932\n",
      "loss: 0.436294  646464/697932\n",
      "loss: 0.472228  652864/697932\n",
      "loss: 0.485863  659264/697932\n",
      "loss: 0.560367  665664/697932\n",
      "loss: 0.565291  672064/697932\n",
      "loss: 0.510486  678464/697932\n",
      "loss: 0.500044  684864/697932\n",
      "loss: 0.357206  691264/697932\n",
      "loss: 0.210424  697664/697932\n",
      "% correct: 84.3  avg loss: 0.480657\n",
      "Epoch 4\n",
      "---\n",
      "loss: 0.405185     64/697932\n",
      "loss: 0.656579   6464/697932\n",
      "loss: 0.400631  12864/697932\n",
      "loss: 0.399846  19264/697932\n",
      "loss: 0.679273  25664/697932\n",
      "loss: 0.378994  32064/697932\n",
      "loss: 0.385526  38464/697932\n",
      "loss: 0.491766  44864/697932\n",
      "loss: 0.446298  51264/697932\n",
      "loss: 0.394184  57664/697932\n",
      "loss: 0.419509  64064/697932\n",
      "loss: 0.209596  70464/697932\n",
      "loss: 0.591927  76864/697932\n",
      "loss: 0.352272  83264/697932\n",
      "loss: 0.549815  89664/697932\n",
      "loss: 0.419993  96064/697932\n",
      "loss: 0.473908  102464/697932\n",
      "loss: 0.469822  108864/697932\n",
      "loss: 0.549285  115264/697932\n",
      "loss: 0.576523  121664/697932\n",
      "loss: 0.598465  128064/697932\n",
      "loss: 0.451949  134464/697932\n",
      "loss: 0.380046  140864/697932\n",
      "loss: 0.511182  147264/697932\n",
      "loss: 0.577991  153664/697932\n",
      "loss: 0.428813  160064/697932\n",
      "loss: 0.461371  166464/697932\n",
      "loss: 0.401738  172864/697932\n",
      "loss: 0.760361  179264/697932\n",
      "loss: 0.453389  185664/697932\n",
      "loss: 0.332151  192064/697932\n",
      "loss: 0.499940  198464/697932\n",
      "loss: 0.523825  204864/697932\n",
      "loss: 0.595355  211264/697932\n",
      "loss: 0.498189  217664/697932\n",
      "loss: 0.366069  224064/697932\n",
      "loss: 0.455497  230464/697932\n",
      "loss: 0.578858  236864/697932\n",
      "loss: 0.377542  243264/697932\n",
      "loss: 0.560553  249664/697932\n",
      "loss: 0.496127  256064/697932\n",
      "loss: 0.515875  262464/697932\n",
      "loss: 0.484180  268864/697932\n",
      "loss: 0.541254  275264/697932\n",
      "loss: 0.418448  281664/697932\n",
      "loss: 0.653193  288064/697932\n",
      "loss: 0.620998  294464/697932\n",
      "loss: 0.449535  300864/697932\n",
      "loss: 0.448152  307264/697932\n",
      "loss: 0.597526  313664/697932\n",
      "loss: 0.688319  320064/697932\n",
      "loss: 0.429428  326464/697932\n",
      "loss: 0.701060  332864/697932\n",
      "loss: 0.516731  339264/697932\n",
      "loss: 0.772530  345664/697932\n",
      "loss: 0.627497  352064/697932\n",
      "loss: 0.551122  358464/697932\n",
      "loss: 0.385748  364864/697932\n",
      "loss: 0.426039  371264/697932\n",
      "loss: 0.625634  377664/697932\n",
      "loss: 0.375759  384064/697932\n",
      "loss: 0.426943  390464/697932\n",
      "loss: 0.418569  396864/697932\n",
      "loss: 0.376505  403264/697932\n",
      "loss: 0.507425  409664/697932\n",
      "loss: 0.444870  416064/697932\n",
      "loss: 0.645156  422464/697932\n",
      "loss: 0.576702  428864/697932\n",
      "loss: 0.349554  435264/697932\n",
      "loss: 0.593522  441664/697932\n",
      "loss: 0.368002  448064/697932\n",
      "loss: 0.406188  454464/697932\n",
      "loss: 0.377111  460864/697932\n",
      "loss: 0.346182  467264/697932\n",
      "loss: 0.369863  473664/697932\n",
      "loss: 0.356544  480064/697932\n",
      "loss: 0.365513  486464/697932\n",
      "loss: 0.379893  492864/697932\n",
      "loss: 0.625228  499264/697932\n",
      "loss: 0.365193  505664/697932\n",
      "loss: 0.248644  512064/697932\n",
      "loss: 0.352302  518464/697932\n",
      "loss: 0.510578  524864/697932\n",
      "loss: 0.501458  531264/697932\n",
      "loss: 0.667455  537664/697932\n",
      "loss: 0.751598  544064/697932\n",
      "loss: 0.298453  550464/697932\n",
      "loss: 0.360744  556864/697932\n",
      "loss: 0.624282  563264/697932\n",
      "loss: 0.371576  569664/697932\n",
      "loss: 0.362827  576064/697932\n",
      "loss: 0.637716  582464/697932\n",
      "loss: 0.459376  588864/697932\n",
      "loss: 0.409119  595264/697932\n",
      "loss: 0.288301  601664/697932\n",
      "loss: 0.378128  608064/697932\n",
      "loss: 0.763146  614464/697932\n",
      "loss: 0.711841  620864/697932\n",
      "loss: 0.457145  627264/697932\n",
      "loss: 0.607679  633664/697932\n",
      "loss: 0.365093  640064/697932\n",
      "loss: 0.428365  646464/697932\n",
      "loss: 0.467268  652864/697932\n",
      "loss: 0.478243  659264/697932\n",
      "loss: 0.559787  665664/697932\n",
      "loss: 0.561409  672064/697932\n",
      "loss: 0.505802  678464/697932\n",
      "loss: 0.501287  684864/697932\n",
      "loss: 0.344832  691264/697932\n",
      "loss: 0.208670  697664/697932\n",
      "% correct: 84.4  avg loss: 0.476177\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for ep in range(EPOCHS):\n",
    "    print(f\"Epoch {ep}\\n---\")\n",
    "    train(training_dataloader, model, loss_func, optimizer)\n",
    "    test(testing_dataloader, model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %sequential.0.weight : Float(16, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.0.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.2.weight : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.2.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.2.running_mean : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %sequential.2.running_var : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %sequential.4.weight : Float(16, 16, 5, 5, strides=[400, 25, 5, 1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.4.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.6.weight : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.6.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.6.running_mean : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %sequential.6.running_var : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %sequential.9.weight : Float(47, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %sequential.9.bias : Float(47, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/sequential/sequential.0/Conv_output_0 : Float(1, 16, 13, 13, strides=[2704, 169, 13, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/sequential/sequential.0/Conv\"](%input.1, %sequential.0.weight, %sequential.0.bias), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.conv.Conv2d::sequential.0 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/sequential/sequential.1/Relu_output_0 : Float(1, 16, 13, 13, strides=[2704, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/sequential/sequential.1/Relu\"](%/sequential/sequential.0/Conv_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.activation.ReLU::sequential.1 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:1473:0\n",
      "  %/sequential/sequential.2/BatchNormalization_output_0 : Float(1, 16, 13, 13, strides=[2704, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002, training_mode=0, onnx_name=\"/sequential/sequential.2/BatchNormalization\"](%/sequential/sequential.1/Relu_output_0, %sequential.2.weight, %sequential.2.bias, %sequential.2.running_mean, %sequential.2.running_var), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.batchnorm.BatchNorm2d::sequential.2 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:2482:0\n",
      "  %/sequential/sequential.3/MaxPool_output_0 : Float(1, 16, 6, 6, strides=[576, 36, 6, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/sequential/sequential.3/MaxPool\"](%/sequential/sequential.2/BatchNormalization_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.pooling.MaxPool2d::sequential.3 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:796:0\n",
      "  %/sequential/sequential.4/Conv_output_0 : Float(1, 16, 2, 2, strides=[64, 4, 2, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/sequential/sequential.4/Conv\"](%/sequential/sequential.3/MaxPool_output_0, %sequential.4.weight, %sequential.4.bias), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.conv.Conv2d::sequential.4 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/sequential/sequential.5/Relu_output_0 : Float(1, 16, 2, 2, strides=[64, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/sequential/sequential.5/Relu\"](%/sequential/sequential.4/Conv_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.activation.ReLU::sequential.5 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:1473:0\n",
      "  %/sequential/sequential.6/BatchNormalization_output_0 : Float(1, 16, 2, 2, strides=[64, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002, training_mode=0, onnx_name=\"/sequential/sequential.6/BatchNormalization\"](%/sequential/sequential.5/Relu_output_0, %sequential.6.weight, %sequential.6.bias, %sequential.6.running_mean, %sequential.6.running_var), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.batchnorm.BatchNorm2d::sequential.6 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:2482:0\n",
      "  %/sequential/sequential.7/MaxPool_output_0 : Float(1, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/sequential/sequential.7/MaxPool\"](%/sequential/sequential.6/BatchNormalization_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.pooling.MaxPool2d::sequential.7 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:796:0\n",
      "  %/sequential/sequential.8/Flatten_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1, onnx_name=\"/sequential/sequential.8/Flatten\"](%/sequential/sequential.7/MaxPool_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.flatten.Flatten::sequential.8 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/modules/flatten.py:49:0\n",
      "  %/sequential/sequential.9/Gemm_output_0 : Float(1, 47, strides=[47, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/sequential/sequential.9/Gemm\"](%/sequential/sequential.8/Flatten_output_0, %sequential.9.weight, %sequential.9.bias), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.linear.Linear::sequential.9 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %27 : Float(1, 47, strides=[47, 1], requires_grad=1, device=cuda:0) = onnx::LogSoftmax[axis=1, onnx_name=\"/sequential/sequential.10/LogSoftmax\"](%/sequential/sequential.9/Gemm_output_0), scope: __main__.TheModel::/torch.nn.modules.container.Sequential::sequential/torch.nn.modules.activation.LogSoftmax::sequential.10 # /home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/nn/functional.py:1947:0\n",
      "  return (%27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "empty_input = torch.zeros(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(model, empty_input, \"onnx_model.onnx\", verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-train-cWreNkmb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
