{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST\n",
      "FashionMNIST\n",
      "KMNIST\n",
      "MNIST\n",
      "MovingMNIST\n",
      "QMNIST\n",
      "mnist\n",
      "moving_mnist\n"
     ]
    }
   ],
   "source": [
    "a = dir(datasets)\n",
    "\n",
    "for text in a:\n",
    "    if \"mnist\" in text.lower():\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset EMNIST\n",
      "    Number of datapoints: 697932\n",
      "    Root location: ./dataset/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: <function TRANSFORM_CALLABLE at 0x7f465386a440>\n",
      "Dataset EMNIST\n",
      "    Number of datapoints: 116323\n",
      "    Root location: ./dataset/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: <function TRANSFORM_CALLABLE at 0x7f465386a440>\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"./dataset/\"\n",
    "\n",
    "# merges letters that have similar upper and lower cases, like C:c and W:w\n",
    "SPLIT_DATA_BY = \"bymerge\"\n",
    "\n",
    "\n",
    "def TRANSFORM_CALLABLE(img: Image.Image) -> torch.Tensor:\n",
    "    return ToTensor()(img.transpose(Image.TRANSPOSE))\n",
    "\n",
    "training_data = datasets.EMNIST(root=ROOT_DIR, split=SPLIT_DATA_BY, train=True, download=True, transform=TRANSFORM_CALLABLE)\n",
    "testing_data = datasets.EMNIST(ROOT_DIR, split=SPLIT_DATA_BY, train=False, download=True, transform=TRANSFORM_CALLABLE)\n",
    "\n",
    "print(training_data)\n",
    "print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAMAElEQVR4nO3dzYvVZR/H8evMjI+N5rNlCC4kyEqyiEgZyQSJglzUol30D4Sb2rZt0SqCoF2RqyCiNlGEZZEtAjNTiEyRcBGOMT5lTqPn3tx87u6HxXx/N54zzbxe0Ebmw7k8o/Oe30RXvX6/328A0FobGfYBAJg7RAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBRaszz//vPV6vf/5zzfffDPs48FQjA37ADBsL774Ynv44Yf/7de2bt06pNPAcIkCC97ExER79tlnh30MmBP8+Ahaa5cvX24zMzPDPgYMnSiw4L3wwgtt5cqVbenSpW3Pnj3t22+/HfaRYGj8+IgFa/Hixe2ZZ55pTz75ZFu3bl07efJke+2119rExET7+uuv244dO4Z9RBi4nv/JDvzLqVOn2vbt29vu3bvbxx9/POzjwMD58RH8xdatW9v+/fvboUOH2o0bN4Z9HBg4UYD/sHnz5jY9Pd2uXr067KPAwIkC/IfTp0+3pUuXtvHx8WEfBQZOFFiwzp8//1+/duzYsfbhhx+2ffv2tZERfz1YePyLZhasxx9/vC1btqzt3LmzbdiwoZ08ebK99dZbbdGiRe3IkSPtnnvuGfYRYeBEgQXr9ddfbwcPHmynTp1qly5dauvXr2979+5tr7zyimsuWLBEAYDwQ1MAQhQACFEAIEQBgBAFAEIUAIhZX53d6/Vu5TkAuMVm818geFIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIixYR8AFpqRkW7fi3XdVd28eXMgG+YmTwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UI8+KfR0dHyZu3ateXNrl27ypvWWrv//vs77aqOHz9e3pw4caK8mZ6eLm+67pYvX17eTE1NlTcXLlwob1prrd/vd9rdCp4UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAMKFeAzMyEi370G67BYtWlTerFixoryZmJgob5577rnyprXBXYj3ww8/lDddLtG7evVqedNaa+fPny9vNm/eXN4cPXq0vPn000/Lm9a6Xw54K3hSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4s1RY2PdPjXj4+PlTZfL41avXl3e3HvvveVNa90ugrv99tvLmy1btpQ3u3btKm/WrVtX3rTW2ujoaHnT7/fLmy7vQ5eLAf/444/yprXWJicny5suf8ZXrlxZ3nz11VflTWsuxANgjhIFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHBL6gB0ud1y06ZNnV7rwIED5U2XWzG3bdtW3qxZs6a8aa21VatWddrNVb/99lun3dTUVHlz6dKl8ubQoUPlzfHjx8uby5cvlzettXbmzJny5sqVK+XNoN7vucaTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEC4EG+OGhnp1uvt27eXN48++mh5s2zZsvKmqy4Xk509e7a8OX36dHnz/fffD2TTWmsnT54sb65fv17e/Prrr+XN9PR0edPv98ub1lq7efNmpx2z40kBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIHr9Wd5K1ev1bvVZ+ItFixZ12m3btq28OXDgQHkzMTFR3mzcuLG8aa21n376qbx55513ypsjR46UN0ePHi1vZmZmypvWXATH/282X+49KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE2LAPwP/2559/dtp1uTzu/fffL28mJyfLmz179pQ3rbV22223lTcPPPBAedPl93T8+PHyZnp6uryBQfGkAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAED0+v1+f1Yf2Ovd6rMwJCMj9e8NlixZUt7ceeed5U1rrT344IPlzfPPP1/erF+/vrz56KOPypuDBw+WN621du7cufKm6227zE+z+XLvSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgXIjHnDc2NlbePPXUU+XN/v37y5u9e/eWN4cPHy5vWmvt1VdfLW9OnDjR6bWYn1yIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgBRv2kMBmxmZqa8+eSTT8qbLpfHrV69urx5+umny5vWWjt27Fh58+OPP5Y3Xd5v5g9PCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDhQjzmpWvXrpU3Z86cKW/efffd8uaRRx4pb1prbWJiorz54IMPypuff/65vOn3++UNc5MnBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDo9Wd5k1Wv17vVZ4G/nQ0bNpQ3b775ZqfX2rdvX3nz3nvvlTcvv/xyeTM5OVneMHiz+XLvSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAGBv2AeDv7MqVK+XN2bNnO73W4sWLy5v77ruvvBkfHy9v3JI6f3hSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIg5dyFer9frtFuzZs1AXuvixYvlTb/fL28GaVDnu3nzZqfd6OhoedPlczs2Vv/rsGnTpvJmy5Yt5U1r3d6HFStWlDdd3gfmD08KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHnbr7qcrFda6299NJL5U2Xy8xOnz5d3ty4caO86XpJ3bVr18qbycnJ8mb16tXlzXfffVfetNbajh07yptVq1aVN8uXLy9v7r777vJm586d5U1r3T63R44cKW+mpqbKG+YPTwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAMecuxOtyeVxrrX355ZflzUMPPVTePPHEE+XNL7/8Ut50uaSutW6/p82bN5c34+Pj5c3FixfLm9ZaW7t2bXnT6/U6vdYgXufcuXOdXuuNN94ob95+++3y5sKFC+UN84cnBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDo9fv9/qw+cEAXjHU1MlLv25IlS8qbjRs3ljdXr14tb65fv17etNbali1bypu9e/eWN3fddVd589hjj5U3XX3xxRflzZUrV8qbLhc4Hj16tLxprbXPPvusvPn99987vRbz02y+3HtSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACDmzS2pgzI6OlrezPIt/r83rXW7+fWOO+4ob9auXVve7N69u7zp6vDhw+XNoG5JnZqaKm9aa+3ChQvlTdc/R8xPbkkFoEQUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAhHgMzNjY2sNeamZkZ2GvB34UL8QAoEQUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgXIgHsEC4EA+AElEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGJsth/Y7/dv5TkAmAM8KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABD/AJiVCJXgq+eFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    rand_ind = torch.randint(len(training_data), size=(1,)).item()\n",
    "    assert isinstance(rand_ind, int)\n",
    "\n",
    "    img, label = training_data[rand_ind]\n",
    "    img_class = training_data.classes[label]\n",
    "    \n",
    "    assert isinstance(img, torch.Tensor) and img.shape == (1, 28, 28)\n",
    "\n",
    "    plt.imshow(img[0], cmap=\"gray\")\n",
    "    plt.title(img_class)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.float32\n",
      "torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "X, y = next(iter(testing_dataloader))\n",
    "\n",
    "print(X.shape, X.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liheng/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModel(\n",
      "  (sequential): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=16, out_features=47, bias=True)\n",
      "    (10): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TheModel(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__() \n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 16, 5, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # nn.Conv2d(16, 16, 3, 2, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            # nn.MaxPool2d(2),\n",
    "\n",
    "            # nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, output_dim),\n",
    "            nn.LogSoftmax(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "model = TheModel(len(training_data.classes)).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= 1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    N = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for ind_batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        forward = model(X)\n",
    "        loss = loss_func(forward, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if ind_batch % 100 == 0:\n",
    "            loss_val, current = loss.item(), (ind_batch + 1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  {current:>5d}/{N:>5d}\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_func):\n",
    "    N = len(dataloader.dataset)\n",
    "    average_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            inference = model(X)\n",
    "            average_loss += loss_func(inference, y).item()\n",
    "            correct += (inference.argmax(1) == y).sum().item()\n",
    "    \n",
    "    average_loss /= len(dataloader)\n",
    "    correct /= N\n",
    "\n",
    "    print(f\"% correct: {(100 * correct):>0.1f}  avg loss: {average_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "---\n",
      "loss: 4.515458     64/697932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.840013   6464/697932\n",
      "loss: 3.547712  12864/697932\n",
      "loss: 3.301851  19264/697932\n",
      "loss: 3.130040  25664/697932\n",
      "loss: 2.844040  32064/697932\n",
      "loss: 2.884674  38464/697932\n",
      "loss: 2.816006  44864/697932\n",
      "loss: 3.085298  51264/697932\n",
      "loss: 2.777089  57664/697932\n",
      "loss: 2.378725  64064/697932\n",
      "loss: 2.362000  70464/697932\n",
      "loss: 2.807441  76864/697932\n",
      "loss: 2.272045  83264/697932\n",
      "loss: 2.494200  89664/697932\n",
      "loss: 2.369204  96064/697932\n",
      "loss: 2.249467  102464/697932\n",
      "loss: 2.165577  108864/697932\n",
      "loss: 2.142003  115264/697932\n",
      "loss: 1.919924  121664/697932\n",
      "loss: 2.190131  128064/697932\n",
      "loss: 2.187688  134464/697932\n",
      "loss: 2.107673  140864/697932\n",
      "loss: 1.903810  147264/697932\n",
      "loss: 2.220697  153664/697932\n",
      "loss: 2.282322  160064/697932\n",
      "loss: 1.955062  166464/697932\n",
      "loss: 1.881606  172864/697932\n",
      "loss: 1.615279  179264/697932\n",
      "loss: 2.051400  185664/697932\n",
      "loss: 1.685832  192064/697932\n",
      "loss: 2.149636  198464/697932\n",
      "loss: 2.144240  204864/697932\n",
      "loss: 2.150355  211264/697932\n",
      "loss: 1.746874  217664/697932\n",
      "loss: 1.723012  224064/697932\n",
      "loss: 2.200701  230464/697932\n",
      "loss: 1.636611  236864/697932\n",
      "loss: 1.362187  243264/697932\n",
      "loss: 1.542772  249664/697932\n",
      "loss: 1.671797  256064/697932\n",
      "loss: 1.779965  262464/697932\n",
      "loss: 1.573977  268864/697932\n",
      "loss: 1.930789  275264/697932\n",
      "loss: 1.798348  281664/697932\n",
      "loss: 1.639384  288064/697932\n",
      "loss: 1.950045  294464/697932\n",
      "loss: 1.578357  300864/697932\n",
      "loss: 1.579684  307264/697932\n",
      "loss: 1.611485  313664/697932\n",
      "loss: 1.709957  320064/697932\n",
      "loss: 1.546742  326464/697932\n",
      "loss: 1.673111  332864/697932\n",
      "loss: 1.561850  339264/697932\n",
      "loss: 1.681473  345664/697932\n",
      "loss: 1.725088  352064/697932\n",
      "loss: 1.458585  358464/697932\n",
      "loss: 1.884891  364864/697932\n",
      "loss: 1.757020  371264/697932\n",
      "loss: 1.870642  377664/697932\n",
      "loss: 1.574468  384064/697932\n",
      "loss: 1.509657  390464/697932\n",
      "loss: 1.360756  396864/697932\n",
      "loss: 1.694095  403264/697932\n",
      "loss: 1.673261  409664/697932\n",
      "loss: 1.329813  416064/697932\n",
      "loss: 1.844392  422464/697932\n",
      "loss: 1.559461  428864/697932\n",
      "loss: 1.445722  435264/697932\n",
      "loss: 1.582502  441664/697932\n",
      "loss: 1.313770  448064/697932\n",
      "loss: 1.241305  454464/697932\n",
      "loss: 1.481603  460864/697932\n",
      "loss: 1.372049  467264/697932\n",
      "loss: 1.237322  473664/697932\n",
      "loss: 1.595374  480064/697932\n",
      "loss: 1.323838  486464/697932\n",
      "loss: 1.503484  492864/697932\n",
      "loss: 1.169386  499264/697932\n",
      "loss: 1.349722  505664/697932\n",
      "loss: 1.171133  512064/697932\n",
      "loss: 1.059581  518464/697932\n",
      "loss: 1.427964  524864/697932\n",
      "loss: 1.575287  531264/697932\n",
      "loss: 1.463668  537664/697932\n",
      "loss: 1.450872  544064/697932\n",
      "loss: 1.108778  550464/697932\n",
      "loss: 1.309433  556864/697932\n",
      "loss: 1.184702  563264/697932\n",
      "loss: 1.479590  569664/697932\n",
      "loss: 1.209015  576064/697932\n",
      "loss: 1.490010  582464/697932\n",
      "loss: 1.107803  588864/697932\n",
      "loss: 1.164908  595264/697932\n",
      "loss: 0.740526  601664/697932\n",
      "loss: 1.522610  608064/697932\n",
      "loss: 1.524450  614464/697932\n",
      "loss: 1.138969  620864/697932\n",
      "loss: 1.128286  627264/697932\n",
      "loss: 1.395317  633664/697932\n",
      "loss: 1.080447  640064/697932\n",
      "loss: 1.194824  646464/697932\n",
      "loss: 1.072972  652864/697932\n",
      "loss: 1.132540  659264/697932\n",
      "loss: 1.410560  665664/697932\n",
      "loss: 1.334743  672064/697932\n",
      "loss: 1.160974  678464/697932\n",
      "loss: 1.091635  684864/697932\n",
      "loss: 1.286273  691264/697932\n",
      "loss: 0.967995  697664/697932\n",
      "% correct: 67.9  avg loss: 1.194150\n",
      "Epoch 1\n",
      "---\n",
      "loss: 1.239443     64/697932\n",
      "loss: 0.953176   6464/697932\n",
      "loss: 1.289193  12864/697932\n",
      "loss: 1.074157  19264/697932\n",
      "loss: 1.284006  25664/697932\n",
      "loss: 0.943905  32064/697932\n",
      "loss: 0.989444  38464/697932\n",
      "loss: 1.188671  44864/697932\n",
      "loss: 1.407287  51264/697932\n",
      "loss: 1.139310  57664/697932\n",
      "loss: 1.030431  64064/697932\n",
      "loss: 0.908603  70464/697932\n",
      "loss: 1.359749  76864/697932\n",
      "loss: 0.972121  83264/697932\n",
      "loss: 1.221746  89664/697932\n",
      "loss: 1.247379  96064/697932\n",
      "loss: 1.098918  102464/697932\n",
      "loss: 1.022718  108864/697932\n",
      "loss: 1.151551  115264/697932\n",
      "loss: 0.961793  121664/697932\n",
      "loss: 1.136291  128064/697932\n",
      "loss: 1.132409  134464/697932\n",
      "loss: 1.132092  140864/697932\n",
      "loss: 1.056673  147264/697932\n",
      "loss: 1.279696  153664/697932\n",
      "loss: 1.137947  160064/697932\n",
      "loss: 1.145015  166464/697932\n",
      "loss: 0.937703  172864/697932\n",
      "loss: 0.849553  179264/697932\n",
      "loss: 1.103937  185664/697932\n",
      "loss: 0.853013  192064/697932\n",
      "loss: 1.329066  198464/697932\n",
      "loss: 1.407814  204864/697932\n",
      "loss: 1.278688  211264/697932\n",
      "loss: 0.984493  217664/697932\n",
      "loss: 1.018297  224064/697932\n",
      "loss: 1.363017  230464/697932\n",
      "loss: 0.986624  236864/697932\n",
      "loss: 0.718303  243264/697932\n",
      "loss: 0.854604  249664/697932\n",
      "loss: 0.942785  256064/697932\n",
      "loss: 1.060318  262464/697932\n",
      "loss: 0.840823  268864/697932\n",
      "loss: 1.229747  275264/697932\n",
      "loss: 1.187077  281664/697932\n",
      "loss: 1.145439  288064/697932\n",
      "loss: 1.243999  294464/697932\n",
      "loss: 1.045471  300864/697932\n",
      "loss: 0.964181  307264/697932\n",
      "loss: 1.053413  313664/697932\n",
      "loss: 1.130702  320064/697932\n",
      "loss: 0.994632  326464/697932\n",
      "loss: 1.213152  332864/697932\n",
      "loss: 1.047735  339264/697932\n",
      "loss: 1.177147  345664/697932\n",
      "loss: 1.106772  352064/697932\n",
      "loss: 1.122371  358464/697932\n",
      "loss: 1.116250  364864/697932\n",
      "loss: 1.183095  371264/697932\n",
      "loss: 1.327865  377664/697932\n",
      "loss: 0.805244  384064/697932\n",
      "loss: 1.071038  390464/697932\n",
      "loss: 0.848173  396864/697932\n",
      "loss: 1.159634  403264/697932\n",
      "loss: 1.099168  409664/697932\n",
      "loss: 0.803764  416064/697932\n",
      "loss: 1.148040  422464/697932\n",
      "loss: 0.985390  428864/697932\n",
      "loss: 0.794252  435264/697932\n",
      "loss: 1.093028  441664/697932\n",
      "loss: 0.874891  448064/697932\n",
      "loss: 0.873919  454464/697932\n",
      "loss: 1.029363  460864/697932\n",
      "loss: 1.050021  467264/697932\n",
      "loss: 0.778102  473664/697932\n",
      "loss: 0.949045  480064/697932\n",
      "loss: 0.801637  486464/697932\n",
      "loss: 1.083575  492864/697932\n",
      "loss: 0.834840  499264/697932\n",
      "loss: 0.988366  505664/697932\n",
      "loss: 0.719592  512064/697932\n",
      "loss: 0.697221  518464/697932\n",
      "loss: 1.053614  524864/697932\n",
      "loss: 1.114027  531264/697932\n",
      "loss: 1.062680  537664/697932\n",
      "loss: 1.063572  544064/697932\n",
      "loss: 0.796650  550464/697932\n",
      "loss: 0.825181  556864/697932\n",
      "loss: 0.870437  563264/697932\n",
      "loss: 0.961732  569664/697932\n",
      "loss: 0.678574  576064/697932\n",
      "loss: 1.245280  582464/697932\n",
      "loss: 0.812767  588864/697932\n",
      "loss: 0.818703  595264/697932\n",
      "loss: 0.577932  601664/697932\n",
      "loss: 1.016753  608064/697932\n",
      "loss: 1.161587  614464/697932\n",
      "loss: 0.769670  620864/697932\n",
      "loss: 0.812972  627264/697932\n",
      "loss: 1.004971  633664/697932\n",
      "loss: 0.716586  640064/697932\n",
      "loss: 0.861028  646464/697932\n",
      "loss: 0.789084  652864/697932\n",
      "loss: 0.784352  659264/697932\n",
      "loss: 0.945192  665664/697932\n",
      "loss: 0.974034  672064/697932\n",
      "loss: 0.819831  678464/697932\n",
      "loss: 0.726924  684864/697932\n",
      "loss: 0.903905  691264/697932\n",
      "loss: 0.654577  697664/697932\n",
      "% correct: 74.8  avg loss: 0.885136\n",
      "Epoch 2\n",
      "---\n",
      "loss: 0.833084     64/697932\n",
      "loss: 0.770658   6464/697932\n",
      "loss: 0.875421  12864/697932\n",
      "loss: 0.810006  19264/697932\n",
      "loss: 1.056705  25664/697932\n",
      "loss: 0.741840  32064/697932\n",
      "loss: 0.715717  38464/697932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test(testing_dataloader, model, loss_func)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_func, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind_batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      9\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     forward \u001b[38;5;241m=\u001b[39m model(X)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mTRANSFORM_CALLABLE\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTRANSFORM_CALLABLE\u001b[39m(img: Image\u001b[38;5;241m.\u001b[39mImage) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRANSPOSE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ocr-train-cWreNkmb/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mByteTensor\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for ep in range(EPOCHS):\n",
    "    print(f\"Epoch {ep}\\n---\")\n",
    "    train(training_dataloader, model, loss_func, optimizer)\n",
    "    test(testing_dataloader, model, loss_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-train-cWreNkmb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
